# Roadmap de D√©ploiement ‚Äî Epics & Livrables Techniques

## Contexte

Ce document d√©taille la roadmap de d√©ploiement en 4 phases (16 semaines) du produit IA self-hosted. Chaque phase est d√©compos√©e en Epics avec leurs livrables techniques attendus. L'ensemble s'inscrit dans l'architecture Kubernetes avec MAX Serve (Mojo), MCP Gateway (MCPJungle), et Memory Engine (Mem0 + Qdrant + Neo4j + Redis).

***

## Phase 1 ‚Äî MVP Fonctionnel (Semaines 1‚Äì4)

> **Objectif** : Un syst√®me op√©rationnel capable d'ing√©rer des documents, stocker en m√©moire vectorielle, et r√©pondre via une API OpenAI-compatible servie par MAX.

***

### Epic 1.1 ‚Äî Infrastructure de Base & Containerisation

Poser le socle Docker Compose local qui sera migr√© vers Kubernetes en Phase 4.

- Provisionnement d'une machine avec GPU (RTX 4090 24GB, 64GB RAM, 500GB NVMe)
- R√©daction du `docker-compose.yaml` ma√Ætre regroupant tous les services (MAX Serve, Qdrant, Redis, PostgreSQL, MinIO)
- Configuration r√©seau Docker : bridge interne `ai-platform-net`, isolation des services
- Script d'initialisation `init.sh` : v√©rification GPU (nvidia-smi), pull des images, cr√©ation des volumes persistants
- Documentation d'installation (README) avec pr√©-requis mat√©riel et logiciel

***

### Epic 1.2 ‚Äî D√©ploiement MAX Serve & Inf√©rence

Rendre le moteur d'inf√©rence Mojo/MAX op√©rationnel avec un premier mod√®le.

- Installation de Modular MAX via `pip install modular`
- D√©ploiement de Llama 3.3 8B Instruct (GGUF Q4_K_M) via `max serve`
- Validation de l'endpoint `/v1/chat/completions` (compatibilit√© OpenAI)
- Configuration du batching (max batch size 32) et des limites m√©moire GPU
- Tests de charge : mesure latency P50/P95/P99, throughput tokens/sec
- Health check endpoint `/health` fonctionnel avec r√©ponse JSON structur√©e

***

### Epic 1.3 ‚Äî Pipeline RAG Core

Construire la cha√Æne compl√®te Ingestion ‚Üí Embedding ‚Üí Retrieval ‚Üí G√©n√©ration.

- D√©ploiement de Qdrant (Docker, volume persistant, port 6333)
- D√©veloppement du service d'ingestion Python : chunking (1 000 caract√®res, overlap 100), metadata tagging (source, date, projet, cat√©gorie)
- Int√©gration du mod√®le d'embedding Nomic Embed v1.5 (servi localement via MAX)
- D√©veloppement du retriever : recherche par similarit√© cosinus, top-k configurable (d√©faut k=5), filtrage par m√©tadonn√©es
- Endpoint API `/v1/rag/query` : re√ßoit une question, retrieves le contexte, g√©n√®re la r√©ponse augment√©e
- Tests unitaires et d'int√©gration sur le pipeline complet (pytest)

***

### Epic 1.4 ‚Äî M√©moire Persistante Mem0

Initialiser la couche m√©moire qui persiste entre les sessions.

- D√©ploiement de Mem0 open-source (Docker) connect√© √† Qdrant comme backend vectoriel
- D√©ploiement Redis (Docker) pour le stockage cl√©-valeur des faits et pr√©f√©rences
- Configuration de l'extraction automatique de faits depuis les conversations
- API m√©moire : `POST /memory/add`, `GET /memory/search`, `DELETE /memory/forget`
- Scoring de pertinence basique : similarit√© cosinus + d√©croissance temporelle
- Tests de persistance : red√©marrage des containers ‚Üí v√©rification de la r√©tention

***

### Epic 1.5 ‚Äî API Unifi√©e & Tests End-to-End

Exposer un point d'entr√©e unique et valider le flux complet.

- Service API gateway l√©ger (FastAPI) : routing vers MAX Serve, RAG, et Mem0
- Authentification basique (API key) pour le MVP
- Endpoint `/v1/chat` unifi√© : int√®gre m√©moire + RAG + inf√©rence en un seul appel
- Collection Postman / fichier `.http` avec tous les sc√©narios de test
- Test end-to-end : ajout d'un document ‚Üí question sur ce document ‚Üí r√©ponse correcte avec contexte
- Benchmark MVP : latence cible < 3s pour une requ√™te RAG compl√®te

***

## Phase 2 ‚Äî Int√©grations & Intelligence (Semaines 5‚Äì8)

> **Objectif** : Connecter les sources de donn√©es externes via MCP, impl√©menter le routage multi-mod√®le, et structurer les connaissances en graphe.

***

### Epic 2.1 ‚Äî MCP Gateway (MCPJungle)

D√©ployer le gateway MCP et enregistrer les premiers serveurs d'outils.

- D√©ploiement MCPJungle (Docker) avec authentification activ√©e
- Enregistrement des serveurs MCP prioritaires : `mcp-github`, `mcp-notion`, `mcp-filesystem`
- Configuration des ACLs : d√©finition des r√¥les agent et des permissions par serveur
- Endpoint unique `/mcp` expos√© et test√© depuis le service d'orchestration
- Activation OpenTelemetry pour le tracing des appels MCP
- Documentation des sch√©mas de chaque tool expos√© (JSON Schema)

***

### Epic 2.2 ‚Äî Serveurs MCP Applicatifs

D√©ployer et configurer les connecteurs vers les outils du quotidien.

- `mcp-github` : acc√®s repos, issues, PRs, commits (token PAT, scope configur√©)
- `mcp-notion` : acc√®s pages, databases, blocs (OAuth integration token)
- `mcp-slack` : lecture de channels, messages, threads (Bot token, event subscriptions)
- `mcp-gmail` : lecture emails, extraction de t√¢ches (OAuth 2.0, scopes read-only)
- `mcp-calendar` : √©v√©nements, deadlines (Google Calendar API, OAuth 2.0)
- Tests d'int√©gration par serveur : appel tool ‚Üí r√©ponse structur√©e ‚Üí validation du sch√©ma

***

### Epic 2.3 ‚Äî Agent Router & Multi-Mod√®le

Impl√©menter la logique de s√©lection automatique du mod√®le optimal par requ√™te.

- D√©ploiement de Qwen 2.5 Coder 7B et DeepSeek R1 7B en parall√®le sur MAX Serve
- D√©veloppement du classifieur d'intent (code, raisonnement, g√©n√©ral, cr√©atif)
- Logique de routing : intent ‚Üí mod√®le (code‚ÜíQwen, reasoning‚ÜíDeepSeek, default‚ÜíLlama)
- Fallback automatique : si le mod√®le principal est surcharg√©, basculer sur le suivant
- M√©triques de routing : compteur par mod√®le, latence par mod√®le, taux de fallback
- Configuration d√©clarative YAML du routing (pas de hardcoding)

***

### Epic 2.4 ‚Äî Knowledge Graph (Neo4j)

Structurer les relations entre projets, concepts, personnes et √©v√©nements.

- D√©ploiement Neo4j Community (Docker, volume persistant, port 7687)
- D√©finition du sch√©ma de graphe : n≈ìuds (Project, Person, Concept, Document, Problem, Lesson), relations (WORKS_ON, RELATES_TO, CAUSED_BY, SOLVED_BY, LEARNED_FROM)
- Pipeline NER (Named Entity Recognition) : extraction automatique d'entit√©s depuis les documents ing√©r√©s
- Pipeline de construction de relations : co-occurrence, extraction explicite, inference
- API graphe : `POST /graph/query` (Cypher), `GET /graph/neighbors/{entity}`
- Int√©gration avec le retriever RAG : enrichissement des r√©sultats vectoriels avec le contexte relationnel du graphe

***

### Epic 2.5 ‚Äî Budget Memory Dynamique

Impl√©menter l'allocation intelligente de contexte par requ√™te.

- Module `MemoryBudgetAllocator` : estimation de complexit√© ‚Üí r√©partition des tokens (working, project, long-term, RAG)
- Scoring Mem0 complet : pertinence (cosinus) √ó importance (fr√©quence) √ó fra√Æcheur (decay exponentiel)
- M√©canisme de promotion/d√©mission : les souvenirs fr√©quemment utiles gagnent en importance, les inutilis√©s d√©clinent
- Configuration des budgets par d√©faut et overrides par workspace
- Tests A/B : comparer la qualit√© des r√©ponses avec budget fixe vs dynamique
- Logging des allocations pour analyse et optimisation future

***

## Phase 3 ‚Äî Learning Automatique (Semaines 9‚Äì12)

> **Objectif** : Rendre le syst√®me auto-√©volutif avec fine-tuning incr√©mental, d√©tection de drift, et boucle de feedback.

***

### Epic 3.1 ‚Äî Pipeline de Collecte Automatique

Automatiser l'ingestion continue depuis toutes les sources connect√©es.

- CronJob de synchronisation : GitHub (toutes les 6h), Notion (toutes les 4h), Slack (toutes les 2h)
- Service de normalisation : conversion de tous les formats en chunks standardis√©s avec m√©tadonn√©es uniformes
- D√©duplication : hash-based (exact) + similarit√© cosinus (near-duplicate, seuil > 0.95)
- File d'attente d'ingestion (Redis Queue) : buffer les documents en cas de pic de collecte
- Dashboard de monitoring : nombre de documents par source, statut d'indexation, erreurs
- Webhook endpoints pour ingestion en temps r√©el (push depuis GitHub, Notion)

***

### Epic 3.2 ‚Äî Fine-Tuning EWC/LoRA Incr√©mental

Impl√©menter le pipeline d'apprentissage continu qui prot√®ge les connaissances existantes.

- Service `learning-engine` (Python, PyTorch) avec calcul de la Fisher Information Matrix sur le dataset courant
- Pipeline LoRA : extraction des interactions/feedbacks de la semaine ‚Üí formatage dataset ‚Üí fine-tune LoRA rank-16
- R√©gularisation EWC : application de la p√©nalit√© \(\frac{\lambda}{2} \sum_i F_i(\theta_i - \theta^*_i)^2\) pendant le fine-tuning
- Sauvegarde des adapters LoRA sur MinIO (S3-compatible) avec versioning s√©mantique (v1.0, v1.1...)
- Hot-swap des adapters LoRA sur MAX Serve sans red√©marrage du service d'inf√©rence
- CronJob Kubernetes `weekly-finetune` : d√©clench√© chaque dimanche 02:00 UTC

***

### Epic 3.3 ‚Äî D√©tection de Drift & Triggers

Monitorer automatiquement la qualit√© et d√©clencher les r√©-apprentissages.

- Module `DriftMonitor` : calcul KL Divergence sur les embeddings des requ√™tes (fen√™tre glissante 7 jours vs 7 jours pr√©c√©dents)
- Module PSI (Population Stability Index) : stabilit√© de la distribution des intents
- Seuils configurables en YAML : `kl_threshold: 0.1`, `psi_threshold: 0.2`, `accuracy_min: 0.75`
- Actions automatiques : drift d√©tect√© ‚Üí alerte Slack + d√©clenchement pipeline fine-tune
- Tableau de bord Grafana : √©volution de la KL Divergence, PSI, et accuracy au fil du temps
- Logging structur√© de chaque d√©tection pour analyse post-mortem

***

### Epic 3.4 ‚Äî Feedback Loop & Scoring de Qualit√©

Capturer le feedback utilisateur pour am√©liorer le mod√®le.

- Boutons üëç/üëé sur chaque r√©ponse dans la Web UI + API `POST /v1/feedback`
- Stockage des feedbacks en PostgreSQL : query, response, model, memory_used, tools_called, rating, timestamp
- Calcul automatique de l'accuracy par mod√®le, par type d'intent, par projet
- Pond√©ration des donn√©es de fine-tuning : les r√©ponses üëç ont un poids 2x, les üëé un poids 0.5x (pour apprendre √† √©viter)
- Export hebdomadaire du dataset de fine-tuning : interactions filtr√©es par qualit√©
- Rapport mensuel automatique : √©volution de la qualit√©, mod√®les les plus performants, sujets probl√©matiques

***

### Epic 3.5 ‚Äî Meta-Learning Cross-Projets

Permettre au syst√®me de s'adapter plus vite √† chaque nouveau projet en apprenant des patterns transversaux.

- Impl√©mentation MAML (Model-Agnostic Meta-Learning) : extraction de t√¢ches par projet, optimisation du point d'initialisation
- Dataset de m√©ta-t√¢ches : chaque projet = une t√¢che, avec ses documents, feedbacks, et patterns
- Pipeline mensuel `meta-learning-update` : CronJob le 1er de chaque mois
- M√©triques d'adaptation : nombre de steps n√©cessaires pour atteindre 80% accuracy sur un nouveau projet (cible : < 10 steps)
- Stockage des meta-weights sur MinIO avec versioning
- Replay buffer pond√©r√© : les √©checs (plans non r√©ussis) re√ßoivent un poids 3x pour renforcer l'apprentissage des erreurs

***

## Phase 4 ‚Äî Production Hardening (Semaines 13‚Äì16)

> **Objectif** : Migrer sur Kubernetes, s√©curiser, observer, et rendre le syst√®me r√©silient pour un fonctionnement 24/7.

***

### Epic 4.1 ‚Äî Migration Kubernetes

Transformer le d√©ploiement Docker Compose en cluster Kubernetes production.

- Installation K3s (lightweight Kubernetes) sur les nodes GPU et CPU
- R√©daction du Helm chart `ai-platform` avec toutes les d√©pendances (voir structure dans le rapport d'architecture)
- StatefulSets pour les services stateful : Qdrant, Neo4j, PostgreSQL, Redis (avec PersistentVolumeClaims)
- Deployments pour les services stateless : MAX Serve, MCPJungle, Agent Router, Mem0, Learning Engine
- HPA (Horizontal Pod Autoscaler) sur MAX Serve : scaling sur `cpu_utilization > 70%` et `inference_queue_depth > 10`
- Pod Disruption Budgets : `minAvailable: 1` sur chaque service critique
- Script de migration : export des donn√©es Docker volumes ‚Üí import dans PVCs Kubernetes

***

### Epic 4.2 ‚Äî API Gateway & S√©curit√©

Impl√©menter le point d'entr√©e s√©curis√© et la gestion des acc√®s.

- D√©ploiement Kong Ingress Controller sur Kubernetes
- Configuration OAuth 2.1 : issuer local (Keycloak) ou externe, tokens JWT RS256
- Rate limiting multi-couche : par IP (100 req/min), par user (1 000 req/h), par workspace (token budget quotidien)
- TLS 1.3 avec Let's Encrypt (cert-manager) et HSTS
- Network Policies Kubernetes : isolation `ai-platform` namespace, whitelist des flux inter-pods
- RBAC Kubernetes : service accounts d√©di√©s par pod, least-privilege
- Secrets management : Kubernetes Secrets chiffr√©s at-rest (ou HashiCorp Vault si disponible)
- Audit log : chaque requ√™te logg√©e avec user, tokens consomm√©s, latence, mod√®le, outils MCP appel√©s

***

### Epic 4.3 ‚Äî Observabilit√© Compl√®te

Mettre en place la visibilit√© totale sur le syst√®me.

- D√©ploiement Prometheus (scrape toutes les 15s) + Grafana (dashboards pr√©-configur√©s)
- D√©ploiement OpenTelemetry Collector : tracing distribu√© de bout en bout (API Gateway ‚Üí Agent Router ‚Üí MAX Serve ‚Üí MCP ‚Üí Memory)
- D√©ploiement Loki : agr√©gation des logs structur√©s JSON de tous les services, r√©tention 90 jours
- Dashboard Grafana ¬´ Platform Overview ¬ª : latence P99, error rate, GPU utilisation, token usage, m√©moire hit rate
- Dashboard Grafana ¬´ Learning Engine ¬ª : KL Divergence, PSI, accuracy, LoRA versions actives
- Dashboard Grafana ¬´ MCP Activity ¬ª : appels par serveur, latence par tool, erreurs
- Alerting : Prometheus AlertManager ‚Üí Slack (latency > 2s, error rate > 1%, GPU > 90%, drift detected, budget exceeded)

***

### Epic 4.4 ‚Äî R√©silience & Fault Tolerance

Garantir le fonctionnement continu m√™me en cas de d√©faillance partielle.

- Circuit breakers sur chaque appel inter-service (timeout 5s, threshold 3 failures, recovery 30s)
- Fallback chain compl√®te : MAX GPU ‚Üí Ollama CPU ‚Üí r√©ponse cache ‚Üí message de d√©gradation gracieuse
- Liveness probes (restart si unhealthy) + Readiness probes (retrait du load balancer si non pr√™t) sur chaque pod
- Anti-affinity rules : r√©partir les replicas de MAX Serve et MCPJungle sur des nodes diff√©rents
- Graceful shutdown : drain des requ√™tes en cours (30s terminationGracePeriodSeconds)
- Backup automatique quotidien : Qdrant snapshots, Neo4j dump, PostgreSQL pg_dump ‚Üí MinIO (r√©tention 30 jours)
- Runbook de disaster recovery : proc√©dure document√©e de restauration compl√®te depuis les backups

***

### Epic 4.5 ‚Äî Validation & Mise en Production

Valider la robustesse du syst√®me avant le go-live.

- Tests de charge : k6 ou Locust, sc√©nario r√©aliste (50 utilisateurs concurrents, mix requ√™tes chat/RAG/MCP)
- Chaos engineering : injection de pannes (kill pod al√©atoire, saturation CPU, network partition) ‚Üí validation des circuit breakers et fallbacks
- Security audit : scan de vuln√©rabilit√©s des images Docker (Trivy), test de p√©n√©tration sur l'API Gateway
- Test de migration de donn√©es : backup complet ‚Üí restauration sur un cluster vierge ‚Üí validation int√©grit√©
- Documentation op√©rationnelle : architecture decision records (ADR), runbooks, proc√©dures d'escalade
- D√©finition des SLOs (Service Level Objectives) : disponibilit√© 99.5%, latence P99 < 2s, perte de donn√©es 0
- Go/No-Go checklist sign√©e : toutes les m√©triques dans les seuils ‚Üí mise en production