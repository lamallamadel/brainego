# Learning Engine Configuration
# Copy this file to .env and customize for your environment

# Service Configuration
LEARNING_ENGINE_HOST=0.0.0.0
LEARNING_ENGINE_PORT=8003

# Model Configuration
BASE_MODEL_PATH=/models/llama-3.3-8b-instruct-q4_k_m.gguf
MODEL_NAME=meta-llama/Llama-3.3-8B-Instruct

# LoRA Configuration
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.05

# EWC Configuration
# Range: 100-1000
# - 100: Light regularization (more plasticity, faster adaptation)
# - 500: Balanced (recommended default)
# - 1000: Strong regularization (more stability, less forgetting)
EWC_LAMBDA=500.0

# Training Configuration
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=2e-4
NUM_TRAIN_EPOCHS=3
MAX_SEQ_LENGTH=2048
WARMUP_STEPS=10

# MinIO Storage Configuration
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_BUCKET=lora-adapters
MINIO_SECURE=false

# PostgreSQL Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=ai_platform
POSTGRES_USER=ai_user
POSTGRES_PASSWORD=ai_password

# Scheduler Configuration
AUTO_TRAIN_ENABLED=true
TRAIN_SCHEDULE_CRON="0 2 * * 1"  # Every Monday at 2 AM
MIN_SAMPLES_FOR_TRAINING=100

# GPU Configuration
NVIDIA_VISIBLE_DEVICES=all
CUDA_VISIBLE_DEVICES=0
