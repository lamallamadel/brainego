#!/usr/bin/env python3
"""
Example: Streaming chat completions (future enhancement)
Note: Streaming is not yet implemented in the current API server
This is a template for future implementation
"""

import httpx
import asyncio
import json


async def stream_chat(messages, max_tokens=500):
    """
    Stream chat completions (placeholder for future implementation).
    
    To implement streaming:
    1. Update api_server.py to support SSE (Server-Sent Events)
    2. Use async generators in FastAPI
    3. Stream tokens as they are generated by MAX Serve
    """
    
    url = "http://localhost:8000/v1/chat/completions"
    
    payload = {
        "model": "llama-3.3-8b-instruct",
        "messages": messages,
        "max_tokens": max_tokens,
        "stream": True  # Enable streaming
    }
    
    async with httpx.AsyncClient(timeout=300.0) as client:
        async with client.stream("POST", url, json=payload) as response:
            async for chunk in response.aiter_lines():
                if chunk:
                    try:
                        # Parse SSE format: "data: {...}"
                        if chunk.startswith("data: "):
                            data = json.loads(chunk[6:])
                            
                            if "choices" in data:
                                delta = data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                
                                if content:
                                    print(content, end="", flush=True)
                    except json.JSONDecodeError:
                        continue


async def main():
    """
    Example usage of streaming API (future feature).
    """
    print("Streaming Chat Example (Future Feature)")
    print("=" * 60)
    print("\nThis is a template for future streaming implementation.")
    print("Currently, the API returns complete responses.\n")
    
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short story about a robot learning to cook."}
    ]
    
    print("Assistant: ", end="", flush=True)
    
    # For now, use non-streaming
    import requests
    response = requests.post(
        "http://localhost:8000/v1/chat/completions",
        json={
            "model": "llama-3.3-8b-instruct",
            "messages": messages,
            "max_tokens": 500
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        content = result["choices"][0]["message"]["content"]
        
        # Simulate streaming effect
        for char in content:
            print(char, end="", flush=True)
            await asyncio.sleep(0.01)
    
    print("\n")


if __name__ == "__main__":
    asyncio.run(main())
