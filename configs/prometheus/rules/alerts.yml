groups:
  - name: latency_alerts
    interval: 30s
    rules:
      # Gateway latency > 2s
      - alert: HighGatewayLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="gateway"}[5m])) > 2
        for: 2m
        labels:
          severity: warning
          component: gateway
        annotations:
          summary: "High gateway latency detected"
          description: "Gateway P95 latency is {{ $value }}s (threshold: 2s)"

      # MCPJungle Gateway latency > 2s
      - alert: HighMCPJungleLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="mcpjungle-gateway"}[5m])) > 2
        for: 2m
        labels:
          severity: warning
          component: mcpjungle
        annotations:
          summary: "High MCPJungle gateway latency detected"
          description: "MCPJungle P95 latency is {{ $value }}s (threshold: 2s)"

      # MAX Serve latency > 2s
      - alert: HighMaxServeLatency
        expr: histogram_quantile(0.95, rate(inference_duration_seconds_bucket{job=~"max-serve-.*"}[5m])) > 2
        for: 2m
        labels:
          severity: warning
          component: max-serve
        annotations:
          summary: "High MAX Serve inference latency"
          description: "MAX Serve {{ $labels.job }} P95 latency is {{ $value }}s (threshold: 2s)"

      # Memory Engine latency > 2s
      - alert: HighMemoryEngineLatency
        expr: histogram_quantile(0.95, rate(memory_operation_duration_seconds_bucket{job="memory-service"}[5m])) > 2
        for: 2m
        labels:
          severity: warning
          component: memory-engine
        annotations:
          summary: "High Memory Engine latency"
          description: "Memory Engine P95 latency is {{ $value }}s (threshold: 2s)"

  - name: error_rate_alerts
    interval: 30s
    rules:
      # Error rate > 1%
      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status=~"5.."}[5m]) /
            rate(http_requests_total[5m])
          ) * 100 > 1
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "High error rate detected"
          description: "{{ $labels.job }} error rate is {{ $value }}% (threshold: 1%)"

      # MAX Serve error rate > 1%
      - alert: HighMaxServeErrorRate
        expr: |
          (
            rate(inference_requests_total{status="error"}[5m]) /
            rate(inference_requests_total[5m])
          ) * 100 > 1
        for: 2m
        labels:
          severity: critical
          component: max-serve
        annotations:
          summary: "High MAX Serve error rate"
          description: "{{ $labels.job }} error rate is {{ $value }}% (threshold: 1%)"

      # MCP error rate > 1%
      - alert: HighMCPErrorRate
        expr: |
          (
            rate(mcp_requests_total{status="error"}[5m]) /
            rate(mcp_requests_total[5m])
          ) * 100 > 1
        for: 2m
        labels:
          severity: critical
          component: mcp
        annotations:
          summary: "High MCP error rate"
          description: "MCP error rate is {{ $value }}% (threshold: 1%)"

  - name: safety_alerts
    interval: 30s
    rules:
      # Baseline threshold: secret leaks must stay at 0
      - alert: SafetySecretLeakIncident
        expr: |
          sum(increase(api_safety_blocked_categories_total{category=~"secret.*|credential.*"}[15m])) > 0
        for: 0m
        labels:
          severity: critical
          component: safety
        annotations:
          summary: "Secret leak incident detected"
          description: "Secret/credential leak baseline breached (threshold: 0 incidents in 15m)"

      # Baseline threshold: unauthorized write attempts must stay at 0
      - alert: UnauthorizedToolWriteDenied
        expr: |
          sum(increase(api_usage_tool_calls_total{status="error",tool_name=~".*(create|update|delete|write|append|modify|post|send|upload|add).*"}[15m])) > 0
        for: 0m
        labels:
          severity: warning
          component: safety
        annotations:
          summary: "Unauthorized tool write denied"
          description: "Unauthorized tool write baseline breached (threshold: 0 denied writes in 15m)"

      # Blocked/safety-denied request ratio spike (>5%)
      - alert: SafetyBlockedRequestSpike
      # Abnormal guardrail block-rate spike compared to rolling baseline.
      - alert: SafetyBlockRateAbnormalSpike
        expr: |
          (
            sum by (deployment) (rate(guardrail_requests_total{decision="blocked"}[5m])) /
            clamp_min(sum by (deployment) (rate(guardrail_requests_total[5m])), 0.001)
          )
          > on(deployment)
          (
            2 * (
              sum by (deployment) (rate(guardrail_requests_total{decision="blocked"}[1h])) /
              clamp_min(sum by (deployment) (rate(guardrail_requests_total[1h])), 0.001)
            )
          )
          and on(deployment)
          (
            sum by (deployment) (rate(guardrail_requests_total{decision="blocked"}[5m])) /
            clamp_min(sum by (deployment) (rate(guardrail_requests_total[5m])), 0.001)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          component: safety
        annotations:
          summary: "Abnormal safety block-rate spike"
          description: "Deployment {{ $labels.deployment }} block rate more than doubled vs 1h baseline and exceeded 5%"

      # Post-deploy safety regression: robustness score drops after LoRA rollout.
      - alert: SafetyScoreDropAfterDeploy
        expr: |
          changes(lora_versions_total[30m]) > 0
          and on(deployment)
          (
            max by (deployment) (jailbreak_robustness_score)
            < on(deployment)
            (max by (deployment) (avg_over_time(jailbreak_robustness_score[24h])) - 0.05)
          )
        for: 10m
        labels:
          severity: critical
          component: safety
        annotations:
          summary: "Post-deploy safety score regression"
          description: "Deployment {{ $labels.deployment }} jailbreak robustness dropped by more than 5 points after rollout"

  - name: policy_and_tool_alerts
    interval: 30s
    rules:
      # PolicyDenied spikes on MCP tool calls (> 5x previous hour baseline and at least 1/min)
      - alert: PolicyDeniedSpike
        expr: |
          (
            sum(rate(api_usage_tool_calls_total{status="error"}[5m]))
            >
            5 * clamp_min(sum(rate(api_usage_tool_calls_total{status="error"}[1h] offset 1h)), 0.01)
          )
          and
          (sum(rate(api_usage_tool_calls_total{status="error"}[5m])) > 0.0167)
        for: 5m
        labels:
          severity: warning
          component: policy
        annotations:
          summary: "PolicyDenied spike detected"
          description: "MCP tool-call error rate spiked >5x vs previous hour baseline (current {{ $value }} errors/sec)"

      # Tool-call error ratio exceeds 5%
      - alert: ToolErrorRateHigh
        expr: |
          (
            sum(rate(api_usage_tool_calls_total{status="error"}[5m])) /
            clamp_min(sum(rate(api_usage_tool_calls_total[5m])), 0.001)
          ) * 100 > 5
        for: 5m
        labels:
          severity: critical
          component: mcp
        annotations:
          summary: "Tool error rate is high"
          description: "MCP tool-call error ratio is {{ $value }}% (threshold: 5%)"

      # Latency regression alert: 15m p95 > 35% higher than 6h baseline
      - alert: APILatencyRegression
        expr: |
          histogram_quantile(0.95, sum by (le) (rate(api_usage_latency_seconds_bucket[15m])))
          >
          1.35 * clamp_min(histogram_quantile(0.95, sum by (le) (rate(api_usage_latency_seconds_bucket[6h] offset 15m))), 0.05)
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "API latency regression detected"
          description: "API p95 latency regressed by >35% versus 6h baseline"

  - name: gpu_alerts
    interval: 30s
    rules:
      # GPU utilization > 90%
      - alert: HighGPUUtilization
        expr: nvidia_gpu_duty_cycle > 90
        for: 5m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: "High GPU utilization"
          description: "GPU {{ $labels.gpu }} utilization is {{ $value }}% (threshold: 90%)"

      # GPU memory > 90%
      - alert: HighGPUMemory
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: gpu
        annotations:
          summary: "High GPU memory usage"
          description: "GPU {{ $labels.gpu }} memory is {{ $value }}% (threshold: 90%)"

      # GPU temperature > 85C
      - alert: HighGPUTemperature
        expr: nvidia_gpu_temperature_celsius > 85
        for: 5m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: "High GPU temperature"
          description: "GPU {{ $labels.gpu }} temperature is {{ $value }}°C (threshold: 85°C)"

  - name: drift_alerts
    interval: 30s
    rules:
      # Drift detected
      - alert: DriftDetected
        expr: drift_score > 0.15
        for: 1m
        labels:
          severity: warning
          component: drift-monitor
        annotations:
          summary: "Model drift detected"
          description: "Drift score is {{ $value }} (threshold: 0.15)"

      # High drift score
      - alert: HighDriftScore
        expr: drift_score > 0.25
        for: 1m
        labels:
          severity: critical
          component: drift-monitor
        annotations:
          summary: "High model drift detected"
          description: "Drift score is {{ $value }} (threshold: 0.25) - immediate action required"

  - name: budget_alerts
    interval: 30s
    rules:
      # Budget exceeded
      - alert: BudgetExceeded
        expr: memory_budget_used_bytes > memory_budget_total_bytes
        for: 1m
        labels:
          severity: critical
          component: memory-budget
        annotations:
          summary: "Memory budget exceeded"
          description: "Memory budget exceeded for {{ $labels.session_id }}"

      # Budget warning (>80%)
      - alert: BudgetWarning
        expr: (memory_budget_used_bytes / memory_budget_total_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: memory-budget
        annotations:
          summary: "Memory budget warning"
          description: "Memory budget is {{ $value }}% for {{ $labels.session_id }} (threshold: 80%)"

  - name: service_health_alerts
    interval: 30s
    rules:
      # Service down
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Service is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes"

      # High request queue depth
      - alert: HighRequestQueueDepth
        expr: request_queue_depth > 100
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High request queue depth"
          description: "{{ $labels.job }} queue depth is {{ $value }} (threshold: 100)"

  - name: database_alerts
    interval: 30s
    rules:
      # PostgreSQL connection pool exhausted
      - alert: PostgreSQLPoolExhausted
        expr: pg_stat_activity_count > 90
        for: 5m
        labels:
          severity: critical
          component: postgres
        annotations:
          summary: "PostgreSQL connection pool exhausted"
          description: "PostgreSQL has {{ $value }} active connections (threshold: 90)"

      # Redis memory high
      - alert: RedisMemoryHigh
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is {{ $value }}% (threshold: 90%)"

  - name: tracing_alerts
    interval: 30s
    rules:
      # High trace error rate
      - alert: HighTraceErrorRate
        expr: |
          (
            rate(otelcol_processor_spans_dropped_total[5m]) /
            rate(otelcol_processor_spans_received_total[5m])
          ) * 100 > 5
        for: 5m
        labels:
          severity: warning
          component: tracing
        annotations:
          summary: "High trace drop rate"
          description: "OpenTelemetry Collector is dropping {{ $value }}% of spans (threshold: 5%)"
