version: '3.8'

services:
  max-serve-llama:
    image: modular/max-serve:latest
    container_name: max-serve-llama
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
      - ./configs:/configs
      - ./logs:/logs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    command: >
      max-serve
      --model-path /models/llama-3.3-8b-instruct-q4_k_m.gguf
      --host 0.0.0.0
      --port 8080
      --max-batch-size 32
      --max-waiting-time 10
      --max-tokens 2048
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  max-serve-qwen:
    image: modular/max-serve:latest
    container_name: max-serve-qwen
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8081:8081"
    volumes:
      - ./models:/models
      - ./configs:/configs
      - ./logs:/logs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    command: >
      max-serve
      --model-path /models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
      --host 0.0.0.0
      --port 8081
      --max-batch-size 32
      --max-waiting-time 10
      --max-tokens 4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  max-serve-deepseek:
    image: modular/max-serve:latest
    container_name: max-serve-deepseek
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8082:8082"
    volumes:
      - ./models:/models
      - ./configs:/configs
      - ./logs:/logs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    command: >
      max-serve
      --model-path /models/deepseek-r1-distill-qwen-7b-q4_k_m.gguf
      --host 0.0.0.0
      --port 8082
      --max-batch-size 32
      --max-waiting-time 10
      --max-tokens 4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  embedding-service:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8003:8003"
    environment:
      - EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - EMBEDDING_SERVICE_HOST=0.0.0.0
      - EMBEDDING_SERVICE_PORT=8003
    command: ["python", "embedding_service.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s


  api-server:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: api-server
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8000:8000"
      - "8001:8001"
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - AGENT_ROUTER_CONFIG=/app/configs/agent-router.yaml
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=documents
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4j_password
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
    depends_on:
      max-serve-llama:
        condition: service_healthy
      max-serve-qwen:
        condition: service_healthy
      max-serve-deepseek:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  gateway:
    build:
      context: .
      dockerfile: Dockerfile.gateway
    container_name: gateway
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9002:9002"
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - AGENT_ROUTER_CONFIG=/app/configs/agent-router.yaml
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=documents
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - API_KEYS=${API_KEYS:-}
    depends_on:
      max-serve-llama:
        condition: service_healthy
      max-serve-qwen:
        condition: service_healthy
      max-serve-deepseek:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  mcpjungle-gateway:
    build:
      context: .
      dockerfile: Dockerfile.mcpjungle
    container_name: mcpjungle-gateway
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9100:9100"
    volumes:
      - ./configs:/app/configs:ro
      - ./workspace:/workspace
    environment:
      - AGENT_ROUTER_CONFIG=/app/configs/agent-router.yaml
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=documents
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - API_KEYS=${API_KEYS:-}
      - ENABLE_TELEMETRY=true
      - OTLP_ENDPOINT=http://jaeger:4317
      - JAEGER_ENDPOINT=jaeger:6831
      - MCP_SERVERS_CONFIG=/app/configs/mcp-servers.yaml
      - MCP_ACL_CONFIG=/app/configs/mcp-acl.yaml
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      - NOTION_API_KEY=${NOTION_API_KEY:-}
    depends_on:
      max-serve-llama:
        condition: service_healthy
      max-serve-qwen:
        condition: service_healthy
      max-serve-deepseek:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
      jaeger:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mem0:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: mem0
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8006:8006"
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=memories
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
    depends_on:
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    command: ["uvicorn", "mem0_service:app", "--host", "0.0.0.0", "--port", "8006"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "4317:4317"
      - "4318:4318"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    volumes:
      - jaeger-data:/badger

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts/postgres:/docker-entrypoint-initdb.d
    environment:
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
      - PGDATA=/var/lib/postgresql/data/pgdata
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ai_user -d ai_platform"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  minio:
    image: minio/minio:latest
    container_name: minio
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin123
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-import:/var/lib/neo4j/import
      - neo4j-plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/neo4j_password
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_heap_initial__size=512M
      - NEO4J_dbms_memory_heap_max__size=1G
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      - NEO4JLABS_PLUGINS=["apoc"]
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:7474"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  data-collection:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: data-collection
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8002:8002"
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - DATA_COLLECTION_PORT=8002
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      - GITHUB_DEFAULT_REPO=${GITHUB_DEFAULT_REPO:-}
      - GITHUB_WEBHOOK_SECRET=${GITHUB_WEBHOOK_SECRET:-}
      - NOTION_API_KEY=${NOTION_API_KEY:-}
      - NOTION_WEBHOOK_SECRET=${NOTION_WEBHOOK_SECRET:-}
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN:-}
      - SLACK_CHANNELS=${SLACK_CHANNELS:-}
      - COLLECTION_CONFIG=/app/configs/collection-schedule.yaml
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    command: python data_collection_service.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  ingestion-worker:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: ingestion-worker
    restart: unless-stopped
    networks:
      - ai-platform-net
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NUM_WORKERS=4
    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      data-collection:
        condition: service_healthy
    command: python worker_service.py

  learning-engine:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: learning-engine
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8003:8003"
    volumes:
      - ./models:/models
      - ./configs:/app/configs:ro
      - ./lora_adapters:/lora_adapters
      - ./fisher_matrices:/fisher_matrices
    environment:
      - LEARNING_ENGINE_HOST=0.0.0.0
      - LEARNING_ENGINE_PORT=8003
      - BASE_MODEL_PATH=/models/llama-3.3-8b-instruct-q4_k_m.gguf
      - MODEL_NAME=meta-llama/Llama-3.3-8B-Instruct
      - LORA_RANK=16
      - LORA_ALPHA=32
      - LORA_DROPOUT=0.05
      - EWC_LAMBDA=500.0
      - BATCH_SIZE=4
      - LEARNING_RATE=2e-4
      - NUM_TRAIN_EPOCHS=3
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - MINIO_BUCKET=lora-adapters
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
      - AUTO_TRAIN_ENABLED=true
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: python learning_engine_service.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  drift-monitor:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: drift-monitor
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8004:8004"
    volumes:
      - ./configs:/app/configs:ro
    environment:
      - DRIFT_MONITOR_HOST=0.0.0.0
      - DRIFT_MONITOR_PORT=8004
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL:-}
      - LEARNING_ENGINE_URL=http://learning-engine:8003
    depends_on:
      postgres:
        condition: service_healthy
      learning-engine:
        condition: service_healthy
    command: python drift_monitor.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  maml-service:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: maml-service
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "8005:8005"
    volumes:
      - ./models:/models
      - ./configs:/app/configs:ro
    environment:
      - MAML_SERVICE_HOST=0.0.0.0
      - MAML_SERVICE_PORT=8005
      - MODEL_NAME=meta-llama/Llama-3.3-8B-Instruct
      - LORA_RANK=16
      - LORA_ALPHA=32
      - LORA_DROPOUT=0.05
      - MAML_INNER_LR=1e-3
      - MAML_OUTER_LR=1e-4
      - MAML_INNER_STEPS=5
      - MAML_OUTER_STEPS=100
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
      - MAML_SCHEDULE_ENABLED=true
      - MAML_CRON_SCHEDULE=0 2 1 * *
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: python maml_service.py
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./configs/grafana/provisioning:/etc/grafana/provisioning
      - ./configs/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=grafana-postgresql-datasource
    depends_on:
      prometheus:
        condition: service_healthy
      postgres:
        condition: service_healthy
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # OpenTelemetry Collector for distributed tracing
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter
      - "13133:13133" # Health check
      - "55679:55679" # zpages
    volumes:
      - ./configs/otel-collector:/etc/otelcol
    command: ["--config=/etc/otelcol/otel-collector.yaml"]
    depends_on:
      jaeger:
        condition: service_started
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:13133"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Loki for log aggregation (90-day retention)
  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "3100:3100"
    volumes:
      - ./configs/loki:/etc/loki
      - loki-data:/loki
    command: -config.file=/etc/loki/loki.yaml
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Promtail for log collection
  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9080:9080"
    volumes:
      - ./configs/promtail:/etc/promtail
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/promtail.yaml
    depends_on:
      loki:
        condition: service_healthy

  # AlertManager for alert routing and Slack webhooks
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9093:9093"
    volumes:
      - ./configs/alertmanager:/etc/alertmanager
      - alertmanager-data:/alertmanager
    environment:
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL:-}
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Redis Exporter for Prometheus
  redis-exporter:
    image: oliver006/redis_exporter:v1.55.0
    container_name: redis-exporter
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis:6379
    depends_on:
      redis:
        condition: service_healthy

  # PostgreSQL Exporter for Prometheus
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://ai_user:ai_password@postgres:5432/ai_platform?sslmode=disable
    depends_on:
      postgres:
        condition: service_healthy

  # NVIDIA GPU Exporter for Prometheus
  nvidia-gpu-exporter:
    image: utkuozdemir/nvidia_gpu_exporter:1.2.0
    container_name: nvidia-gpu-exporter
    restart: unless-stopped
    networks:
      - ai-platform-net
    ports:
      - "9835:9835"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]

  # Automated Backup Service
  backup-service:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: backup-service
    restart: unless-stopped
    networks:
      - ai-platform-net
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./backups:/backups
    environment:
      - BACKUP_SCHEDULE=0 2 * * *
      - BACKUP_RETENTION_DAYS=30
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - BACKUP_BUCKET=backups
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - NEO4J_HOST=neo4j
      - NEO4J_PORT=7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4j_password
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
      - RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - RAG_EMBEDDING_PROVIDER=service
      - RAG_EMBEDDING_SERVICE_URL=http://embedding-service:8003
    depends_on:
      qdrant:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    command: python backup_service.py

networks:
  ai-platform-net:
    driver: bridge
    name: ai-platform-net

volumes:
  qdrant-storage:
    driver: local
  redis-data:
    driver: local
  postgres-data:
    driver: local
  minio-data:
    driver: local
  jaeger-data:
    driver: local
  neo4j-data:
    driver: local
  neo4j-logs:
    driver: local
  neo4j-import:
    driver: local
  neo4j-plugins:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  loki-data:
    driver: local
  alertmanager-data:
    driver: local
